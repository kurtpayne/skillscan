from __future__ import annotations

import hashlib
import re
import tempfile
import urllib.error
import urllib.parse
import urllib.request
from dataclasses import dataclass
from pathlib import Path

SOURCE_EXTENSIONS = {
    ".md",
    ".txt",
    ".py",
    ".sh",
    ".ps1",
    ".js",
    ".ts",
    ".json",
    ".yaml",
    ".yml",
    ".toml",
    ".ini",
    ".cfg",
}

MARKDOWN_LINK_RE = re.compile(r"\[[^\]]+\]\(([^)]+)\)")
RAW_URL_RE = re.compile(r"https?://[^\s\"'<>]+")


@dataclass
class RemoteFetchResult:
    root: Path
    cleanup_dir: tempfile.TemporaryDirectory[str]
    unreadable_urls: list[str]
    skipped_urls: list[str]


class RemoteFetchError(Exception):
    pass


def is_url_target(target: str) -> bool:
    parsed = urllib.parse.urlparse(target)
    return parsed.scheme in {"http", "https"} and bool(parsed.netloc)


def _to_raw_github_url(url: str) -> str:
    parsed = urllib.parse.urlparse(url)
    if parsed.netloc != "github.com":
        return url
    parts = [p for p in parsed.path.split("/") if p]
    if len(parts) >= 5 and parts[2] == "blob":
        owner, repo, _, branch = parts[:4]
        rel = "/".join(parts[4:])
        return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rel}"
    return url


def _fetch_text(url: str, timeout_seconds: int = 12) -> str:
    req = urllib.request.Request(url, headers={"User-Agent": "skillscan/0.1"})
    with urllib.request.urlopen(req, timeout=timeout_seconds) as resp:
        payload = resp.read()
    if isinstance(payload, str):
        return payload
    if isinstance(payload, bytes):
        return payload.decode("utf-8", errors="ignore")
    return str(payload)


def _looks_like_source_url(url: str) -> bool:
    parsed = urllib.parse.urlparse(url)
    suffix = Path(parsed.path).suffix.lower()
    return suffix in SOURCE_EXTENSIONS


def _extract_links(text: str, base_url: str) -> list[str]:
    links: set[str] = set()
    for link in MARKDOWN_LINK_RE.findall(text):
        resolved = urllib.parse.urljoin(base_url, link)
        links.add(resolved)
    for link in RAW_URL_RE.findall(text):
        links.add(link)
    out = [u for u in links if _looks_like_source_url(urllib.parse.urldefrag(u).url)]
    return sorted(out)


def _safe_name(url: str) -> str:
    parsed = urllib.parse.urlparse(url)
    stem = Path(parsed.path).name or "remote.txt"
    digest = hashlib.sha256(url.encode("utf-8")).hexdigest()[:12]
    return f"{digest}_{stem}"


def fetch_remote_target(
    url: str,
    max_links: int = 25,
    timeout_seconds: int = 12,
    same_origin_only: bool = True,
) -> RemoteFetchResult:
    root_url = _to_raw_github_url(url)
    try:
        root_text = _fetch_text(root_url, timeout_seconds=timeout_seconds)
    except urllib.error.URLError as exc:
        raise RemoteFetchError(f"Unable to fetch URL target: {url} ({exc})") from exc

    temp = tempfile.TemporaryDirectory(prefix="skillscan-url-")
    root = Path(temp.name)
    root_file = root / _safe_name(root_url)
    root_file.write_text(root_text, encoding="utf-8")

    unreadable: list[str] = []
    skipped: list[str] = []
    links = _extract_links(root_text, base_url=root_url)
    root_origin = urllib.parse.urlparse(root_url).netloc.lower()
    for link in links[:max_links]:
        resolved = _to_raw_github_url(urllib.parse.urldefrag(link).url)
        if same_origin_only:
            link_origin = urllib.parse.urlparse(resolved).netloc.lower()
            if link_origin != root_origin:
                skipped.append(resolved)
                continue
        try:
            text = _fetch_text(resolved, timeout_seconds=timeout_seconds)
        except (urllib.error.URLError, ValueError):
            unreadable.append(resolved)
            continue
        out = root / _safe_name(resolved)
        out.write_text(text, encoding="utf-8")

    return RemoteFetchResult(
        root=root,
        cleanup_dir=temp,
        unreadable_urls=unreadable,
        skipped_urls=skipped,
    )
